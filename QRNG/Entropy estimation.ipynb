{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "070f0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of Case 1 matrix = 999948 x 9.0\n",
      "Entropy of Case 1 = 8.236792836057962\n",
      "Dimension of Case 2 matrix = 999980 x 7.0\n",
      "Entropy of Case 2 = 5.121613938656934\n",
      "Dimension of toeplitz matrix = 90 x 31\n",
      "Dimension of final matrix = 99990 x 31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "\n",
    "#----------Read from the file, find the entropy -- CASE 1\n",
    "\n",
    "\n",
    "colnames=['a']\n",
    "df_1 = pd.read_csv(\"Case_1_binary.txt\", names=colnames, header=None, dtype=str) #Read the .txt file using pandas dataframe (=pd.DF)\n",
    "#print(df_1)\n",
    "\n",
    "x_1 = len(df_1.axes[0]) #Find the number of rows in the pd.DF, required for probability and entropy\n",
    "col_1 = df_1['a'].iloc[0]\n",
    "len_col_1 = len(10*col_1)\n",
    "print(\"Dimension of Case 1 matrix =\",x_1,\"x\",len_col_1/10)\n",
    "x_1_mod = np.mod(x_1, len_col_1)\n",
    "#print(x_1_mod)\n",
    "df_1.drop(df_1.tail(x_1_mod).index,inplace=True)\n",
    "#print(df_1)\n",
    "\n",
    "counts_1 = df_1.value_counts() #Count the repitition/occurance (entries in rows) of each of the possible bit strings\n",
    "#print(counts)\n",
    "\n",
    "df2_1 = pd.DataFrame(counts_1, columns=list('a')) #Update pd.DF column with the rows being unique\n",
    "\n",
    "prob_1 = counts_1/x_1 #Probability of each possible bit string\n",
    "#print(prob)\n",
    "df2_1['b'] = pd.DataFrame(prob_1) #Update pd.DF with prob as a new column of each row\n",
    "\n",
    "df2_1['c'] = -(df2_1['b'] * np.log2(df2_1['b'])) #Find the entropy of the row elements and store in a new column\n",
    "df3_1 = df2_1.sort_index() #Sort the rows with increasing bit string values\n",
    "r_df3_1 = len(df3_1.axes[0]) #Find the number of rows of the pd.DF \n",
    "c_df3_1 = len(df3_1.axes[1]) #Find the number of columns of the pd.DF\n",
    "#print(\"rows=\",r_df3,\", cols=\",c_df3)\n",
    "#df3['0'] = df3['0'].apply('{:0>8}'.format)\n",
    "#print(df2)\n",
    "#print(df3)\n",
    "total_1 = (np.sum(df2_1['c'].values)) #Find the Shannon entropy (sum of individual entropies), required for hashing\n",
    "print(\"Entropy of Case 1 =\",total_1)\n",
    "\n",
    "#df2[\"b\"].plot()\n",
    "#df3[\"b\"].plot()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#----------Read from the file, find the entropy -- CASE 2\n",
    "\n",
    "\n",
    "colnames=['a'] \n",
    "df_2 = pd.read_csv(\"Case_2_binary.txt\", names=colnames, header=None, dtype=str) #Read the .txt file using pandas dataframe (=pd.DF)\n",
    "#print(df_1)\n",
    "\n",
    "#df_1 = dfa_1.head(450002)\n",
    "\n",
    "x_2 = len(df_2.axes[0]) #Find the number of rows in the pd.DF, required for probability and entropy\n",
    "col_2 = df_2['a'].iloc[0]\n",
    "len_col_2 = len(10*col_2)\n",
    "print(\"Dimension of Case 2 matrix =\",x_2,\"x\",len_col_2/10)\n",
    "x_2_mod = np.mod(x_2, len_col_2)\n",
    "#print(x_1_mod)\n",
    "df_2.drop(df_2.tail(x_2_mod).index,inplace=True)\n",
    "#print(df_1)\n",
    "\n",
    "counts_2 = df_2.value_counts() #Count the repitition/occurance (entries in rows) of each of the possible bit strings\n",
    "#print(counts)\n",
    "\n",
    "df2_2 = pd.DataFrame(counts_2, columns=list('a')) #Update pd.DF column with the rows being unique\n",
    "\n",
    "prob_2 = counts_2/x_2 #Probability of each possible bit string\n",
    "#print(prob)\n",
    "df2_2['b'] = pd.DataFrame(prob_2) #Update pd.DF with prob as a new column of each row\n",
    "\n",
    "df2_2['c'] = -(df2_2['b'] * np.log2(df2_2['b'])) #Find the entropy of the row elements and store in a new column\n",
    "df3_2 = df2_2.sort_index() #Sort the rows with increasing bit string values\n",
    "r_df3_2 = len(df3_2.axes[0]) #Find the number of rows of the pd.DF \n",
    "c_df3_2 = len(df3_2.axes[1]) #Find the number of columns of the pd.DF\n",
    "#print(\"rows=\",r_df3,\", cols=\",c_df3)\n",
    "#df3['0'] = df3['0'].apply('{:0>8}'.format)\n",
    "#print(df2)\n",
    "#print(df3)\n",
    "total_2 = (np.sum(df2_2['c'].values)) #Find the Shannon entropy (sum of individual entropies), required for hashing\n",
    "print(\"Entropy of Case 2 =\",total_2)\n",
    "\n",
    "#df2[\"b\"].plot()\n",
    "#df3[\"b\"].plot()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#----------Using the value of entropy, find the dimensionality of the required topelitz matrix and create one -- CASE 1\n",
    "\n",
    "\n",
    "#c_tmat=int(total*10)\n",
    "c_tmat_1=int(10*(total_1-total_2)) #Find the floor of the entropy value, to be used for rows-length in toeplitz matrix\n",
    "#r_tmat1=int(input(\"Enter the string length from ADC : \")) #Input the length of the bits from ADC\n",
    "len_rmat_1 = df_1['a'].iloc[0]\n",
    "r_tmat_1 = len(10*len_rmat_1)\n",
    "#print(r_tmat_1, c_tmat_1)\n",
    "\n",
    "randomlist_1 = []\n",
    "for i_1 in range(0, r_tmat_1):\n",
    "    n_1 = random.randint(0,1)\n",
    "    randomlist_1.append(n_1)\n",
    "#print(randomlist_1) #Create a list of pseudo-random numbers (=PRN) of length equal to that of row # of pd.DF, required for tmat\n",
    "\n",
    "tmat_full_1 = toeplitz(randomlist_1) #Create toeplitz matrix (=tmat) from the list of PRN\n",
    "#print(tmat_full)\n",
    "np.savetxt(\"Case_1_toep_mat_full.txt\",tmat_full_1, delimiter=\",\", fmt='%s')\n",
    "\n",
    "tmat_1 = np.delete(tmat_full_1, np.s_[c_tmat_1:r_tmat_1], axis=1) #Reduce the size of tmat to the required quantity\n",
    "#print(tmat_1)\n",
    "np.savetxt(\"Case_1_toep_mat.txt\",tmat_1, delimiter=\",\", fmt='%s')\n",
    "\n",
    "df_tmat_1 = pd.DataFrame(tmat_1, index=None) #Create a DF of the reduced toeplitz matrix (=t.DF)\n",
    "r_df_tmat_1 = len(df_tmat_1.axes[0]) #Find the # of rows in the t.DF\n",
    "c_df_tmat_1 = len(df_tmat_1.axes[1]) #Find the # of columns in the t.DF\n",
    "print(\"Dimension of toeplitz matrix =\",r_df_tmat_1,\"x\",c_df_tmat_1)\n",
    "#print(df_tmat_1)\n",
    "\n",
    "\n",
    "#-----------------------------------Join 10 lines in list and then perform matmul -- CASE 1\n",
    "\n",
    "\n",
    "row_merged_1 = df_1.groupby(df_1.index//10).agg(list)\n",
    "row_merged_1['a'] = row_merged_1['a'].apply(lambda x: ''.join(map(str, x)))\n",
    "#print(row_merged)\n",
    "np.savetxt(\"Case_1_row_merged.txt\",row_merged_1, delimiter=\"\", fmt='%s')\n",
    "\n",
    "\n",
    "#----------Multiply the n-bit string list file with the toeplitz matrix to get reduced dimensional matrix -- CASE 1\n",
    "\n",
    "\n",
    "result_list_1=[]\n",
    "\n",
    "r_df_1 = int(1)\n",
    "#print(r_df)\n",
    "list_r_df_1 = list(range(r_df_1))\n",
    "#print(list_r_df)\n",
    "\n",
    "df_split_1 = np.array_split(row_merged_1, r_df_1)\n",
    "#df_split_list=df_split.values.tolist()\n",
    "#print(df_split_1)\n",
    "\n",
    "for y in list_r_df_1:\n",
    "    list_df_split_1 = (df_split_1[y]['a']).to_list()\n",
    "    list_df_splitn_1 = list(list_df_split_1)\n",
    "    for i_1 in list_df_splitn_1:\n",
    "        list_final_1 = list(i_1)\n",
    "        list_final2_1 = np.array(list_final_1)\n",
    "        list_final3_1 = pd.to_numeric(list_final2_1)\n",
    "        matrix_multiply_1 = np.matmul(list_final3_1, tmat_1)\n",
    "        binary_matrix_1 = np.mod(matrix_multiply_1,2)\n",
    "        #print(list_final3_1)\n",
    "        #print(matrix_multiply_1)\n",
    "        #print(binary_matrix_1)\n",
    "        result_list_1.append(binary_matrix_1)\n",
    "\n",
    "#print(result_list_1)\n",
    "r_result_1 = len(result_list_1)\n",
    "c_result_1 = len(result_list_1[0])\n",
    "print(\"Dimension of final matrix =\",r_result_1,\"x\",c_result_1)\n",
    "np.savetxt(\"Case_1_reduced_matrix.txt\",result_list_1, delimiter=\"\", fmt='%s')\n",
    "\n",
    "\n",
    "#----------------------------------Check for trimming first/last few bits in the 12 bit string -- CASE 1\n",
    "\n",
    "\n",
    "#row_merged_1['b'] = row_merged_1['a'].str[5:]\n",
    "#np.savetxt(\"Case_1_trimmed_matrix.txt\",row_merged_1['a'], delimiter=\"\", fmt='%s')\n",
    "#print(row_merged)\n",
    "\n",
    "\n",
    "#-----------------------------------Find the min-entropy of reduced matrix -- CASE 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------Find randomness using NIST suite of all the three quantities -- CASE 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "394d7077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of Case 3 = 16.5812172262855\n"
     ]
    }
   ],
   "source": [
    "colnames=['a'] \n",
    "df_3 = pd.read_csv(\"Case_1_reduced_matrix.txt\", names=colnames, header=None, dtype=str) #Read the .txt file using pandas dataframe (=pd.DF)\n",
    "#print(df_1)\n",
    "\n",
    "#df_1 = dfa_1.head(450002)\n",
    "\n",
    "x_3 = len(df_3.axes[0]) #Find the number of rows in the pd.DF, required for probability and entropy\n",
    "col_3 = df_3['a'].iloc[0]\n",
    "len_col_3 = len(10*col_3)\n",
    "#print(x_1, y_1)\n",
    "x_3_mod = np.mod(x_3, len_col_3)\n",
    "#print(x_1_mod)\n",
    "df_3.drop(df_3.tail(x_3_mod).index,inplace=True)\n",
    "#print(df_1)\n",
    "\n",
    "counts_3 = df_3.value_counts() #Count the repitition/occurance (entries in rows) of each of the possible bit strings\n",
    "#print(counts)\n",
    "\n",
    "df2_3 = pd.DataFrame(counts_3, columns=list('a')) #Update pd.DF column with the rows being unique\n",
    "\n",
    "prob_3 = counts_3/x_3 #Probability of each possible bit string\n",
    "#print(prob)\n",
    "df2_3['b'] = pd.DataFrame(prob_3) #Update pd.DF with prob as a new column of each row\n",
    "\n",
    "df2_3['c'] = -(df2_3['b'] * np.log2(df2_3['b'])) #Find the entropy of the row elements and store in a new column\n",
    "df3_3 = df2_3.sort_index() #Sort the rows with increasing bit string values\n",
    "r_df3_3 = len(df3_3.axes[0]) #Find the number of rows of the pd.DF \n",
    "c_df3_3 = len(df3_3.axes[1]) #Find the number of columns of the pd.DF\n",
    "#print(\"rows=\",r_df3,\", cols=\",c_df3)\n",
    "#df3['0'] = df3['0'].apply('{:0>8}'.format)\n",
    "#print(df2)\n",
    "#print(df3)\n",
    "total_3 = (np.sum(df2_3['c'].values)) #Find the Shannon entropy (sum of individual entropies), required for hashing\n",
    "print(\"Entropy of Case 3 =\",total_3)\n",
    "\n",
    "#df2[\"b\"].plot()\n",
    "#df3[\"b\"].plot()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72611595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
